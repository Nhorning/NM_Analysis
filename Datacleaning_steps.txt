
Steps to clean data in Nepal Monitor data set: 


Data is already in flat file format. 

Plan:

1. For their own analysis NM has provided a cleaned up version of the 2017 data certified to have no duplicates. Compare the data in the No duplicates 2017 file to the standard export.  What are the differences? 

2. Remove any duplicates from the 2016 data 
    Find events in the same within the same place and time, and review manually. 
    
3. Append 2017 and 2016 data to ensure a dataset with no duplicates.

4. The UNRCO has provided a new system of place codes, which converts the old system of HLCIT codes used to identify political boundaries down to the ward level into the new official Federal boundaries. Map the clean data frames to the new federal structures. (also do this for the population by ward data and update it on HDX)


Actual steps: 

1. Data import:  
   Remove Errant comma from line 6055 of 1519759656_39328483.csv and save new file with coherent file name - NM2016_17export.csv
   Attempt re-inport
   Check in new files
   
  
2. Remove normalize times so we are only working with dates
   Re index dfs by incident number and sort by index
   write function to verify the dates are the same where the incident numbers are the same. 
   
3. Created function to find the diff between the data frames to see what was dropped and what was added. 
     
4. Created a function to try and flag duplicates by location. Only found one genuine duplicate, description was the same. Steps 1 and 2 in plan are apparently more data engineering than data science.  
 
5. Went back to basic data cleaning steps:
    
    Remove unnecessary columns

    Convert yes no values to 1's and zeros

    Unpack numerical codes into columns

  Found that numerical codes were to numerous to break out into own columns,
 
6. Created function to map new federal structure data to dataset. Found additional cleaning of Federal structure data necessary, due to untidy ward listing
